---
title: "Llama 3.2"
author: "Meta"
description: "Llama 3.2 is Metaâ€™s open-source AI model, optimized for edge devices with multimodal capabilities, enabling text and image processing for broader AI applications across platforms."
---

#  Llama 3.2

Llama 3.2 is Metaâ€™s latest advancement in open-source large language models (LLMs), designed to make AI more accessible across various platforms and tasks, especially with its new multimodal capabilities. This version focuses on lightweight models optimized for edge devices, while also introducing the ability to process both text and images, broadening the scope of AI applications.

| General              |                                                         |
| -------------------- | ------------------------------------------------------- |
| Author               | Meta                                                    |
| Release date         | September 2024                                          |
| Website              | https://www.llama.com/                                  |
| Documentation        | https://www.llama.com/docs/overview                     |
| Collection           | [Llama 3.2 meta-llama Collection](https://huggingface.co/collections/meta-llama/llama-32-66f448ffc8c32f949b04c8cf)                    |
| Model Sizes          | 1B, 3B, 11B, 90B parameters                             |
| Technology Type      | Large Language Model (LLM), Multimodal                  |


## Key Features

- **Multimodal Processing:** Llama 3.2 can handle both text and image inputs, making it useful for visual understanding tasks such as document analysis, image captioning, and visual question answering.

- **Lightweight Models for Edge Devices:** The 1B and 3B parameter models are optimized for mobile and IoT devices, allowing for real-time AI applications on low-powered hardware. These models support a context length of 12K tokens and are compatible with hardware from Qualcomm and MediaTek, making them versatile for edge deployments.

- **Vision-Centric Models:** The 11B and 90B models introduce vision capabilities to Llama, enabling advanced applications like augmented reality (AR) and complex image recognition.

- **On-Device AI:** These models are specifically designed to run efficiently on ARM-based devices, bringing powerful AI capabilities to mobile and edge environments without needing extensive cloud infrastructure.


## Applications

-	**Multimodal AI Tasks:** Llama 3.2â€™s multimodal capabilities allow it to analyze both text and images, which opens up opportunities in fields like:

- **Document Analysis:** Automatically process and extract information from scanned documents.

- **Image Captioning and Object Recognition:** Generate descriptions or identify objects within images.

- **Visual Question Answering:** Answer questions based on visual inputs, making it a valuable tool for accessibility and automation in various industries.

- **On-Device AI:** Due to its lightweight architecture, Llama 3.2 can be deployed on mobile devices or IoT systems for real-time processing, even in environments with limited resources or no internet connection.

- **AR and Vision-Based Applications:** Developers can integrate Llama 3.2 into augmented reality systems, where quick image recognition or contextual understanding is essential.


## Start Building with Llama 3.2

Getting started with Llama 3.2 is easy, whether you're a seasoned developer or just starting out with AI. Meta provides a comprehensive set of resources, including detailed documentation, setup guides, and tutorials to help you integrate Llama 3.2 into your applications. You can choose from various model sizes depending on your use case, whether itâ€™s running locally on your device or deploying in a large-scale cloud environment. Llama 3.2â€™s open-source nature allows for customization and fine-tuning for specialized needs.

ðŸ‘‰ [Start building with Llama 3.2](https://www.llama.com/docs/model-cards-and-prompt-formats/llama3_2)
